# Tokenizer

1. Tokenization이란
   - Tokenization은 입력된 문장을 "토큰"으로 자르는 것을 말한다.

2. Tokenization을 하는 이유
   - OOV를 줄이기 위해서이다.
   - OOV가 많으면 문장의 **분해력**이 줄어든다.
   - OOV는 주로 신조어나 고유명사 등으로 중요한 단어이다.
   - 이를 모두 OOV 토큰으로 처리하면 다음과 같은 문제점이 발생한다.
     1. 중요한 단어를 모델이 인지하지 못한다.
     2. 모두 다른 토큰임에도 같은 값(이를 테면 <unk>)으로 매핑이 된다.

3. Token의 단위  
   <----------------------------------------------------->  
   character &nbsp; &nbsp; **token**   &nbsp; &nbsp; &nbsp;   morph  &nbsp; &nbsp;     word     &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp;      sentence

   - OOV가 발생하지 않도록 다음과 같이 token의 단위를 고민해볼 수 있다.
     - character 단위나, sentence 그대로 저장한다. → 입력의 거의 모든 것을 저장하는 건 비효율적이다(메모리 소모가 커지고, 계산량이 많아짐)
     - 의미 있는 단위(형태소)로 자른다. → *위와 동일*
     - 간단하게 단어 단위로 자른다. → 단어의 기준을 고려해야 함
   - NLP에서 언어 모델은 패턴을 기억한다.
   - 여기서 패턴이라는 건 **문맥**이다.
   - 너무 작은 단위로 토큰을 정하면 패턴이 뭉개질 가능성이 있다.
     - 명확하지 않고 구분되지 않음
